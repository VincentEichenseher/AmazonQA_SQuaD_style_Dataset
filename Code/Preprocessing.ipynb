{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Aufbereiten der Datensätze</h2>\n",
        "\n",
        "Es werden Die Span-based Amazon Datensätze von Gupta et. al (2019) (https://arxiv.org/pdf/1908.04364.pdf) verwendet (siehe https://github.com/amazonqa/amazonqa), die auf den bekannten Amazon QA Datensätzen von Julian Mcauley (2016) (https://cseweb.ucsd.edu//~jmcauley/pdfs/sigir15.pdf) basieren (siehe http://jmcauley.ucsd.edu/data/amazon/qa/).\n",
        "\n",
        "Der TrainingsDatensatz noch in das SQuaD Format gebracht werden, sodass wir ein Modell, das auf bereits auf dem SQuaD datensatz trainiert wurde, mit den Amazon Daten weiter finetunen können.\n"
      ],
      "metadata": {
        "id": "NlQC-aFrhVhN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install git+https://github.com/deepset-ai/haystack.git#egg=farm-haystack[colab]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3N0jzbjof2t",
        "outputId": "0d47408f-0fb6-40d9-b919-ff9883720fc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (21.1.3)\n",
            "Collecting pip\n",
            "  Downloading pip-22.2.2-py3-none-any.whl (2.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0 MB 26.8 MB/s \n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 21.1.3\n",
            "    Uninstalling pip-21.1.3:\n",
            "      Successfully uninstalled pip-21.1.3\n",
            "Successfully installed pip-22.2.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting farm-haystack[colab]\n",
            "  Cloning https://github.com/deepset-ai/haystack.git to /tmp/pip-install-kdwqxqy1/farm-haystack_ab3bbf7a8180432eae4cf7451a41ae43\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/deepset-ai/haystack.git /tmp/pip-install-kdwqxqy1/farm-haystack_ab3bbf7a8180432eae4cf7451a41ae43\n",
            "  Resolved https://github.com/deepset-ai/haystack.git to commit b579b9d54a4e55d6f1d19c16605464b975dadff3\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting azure-ai-formrecognizer>=3.2.0b2\n",
            "  Downloading azure_ai_formrecognizer-3.2.0-py3-none-any.whl (228 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m228.4/228.4 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from farm-haystack[colab]) (2.23.0)\n",
            "Collecting tika\n",
            "  Downloading tika-1.24.tar.gz (28 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.7/dist-packages (from farm-haystack[colab]) (8.14.0)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.7/dist-packages (from farm-haystack[colab]) (1.7.3)\n",
            "Collecting elastic-apm\n",
            "  Downloading elastic_apm-6.12.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (381 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m381.4/381.4 kB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mlflow\n",
            "  Downloading mlflow-1.29.0-py3-none-any.whl (16.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.9/16.9 MB\u001b[0m \u001b[31m65.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentence-transformers>=2.2.0\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting seqeval\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting mmh3\n",
            "  Downloading mmh3-3.0.0-cp37-cp37m-manylinux2010_x86_64.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from farm-haystack[colab]) (4.12.0)\n",
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m66.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting quantulum3\n",
            "  Downloading quantulum3-0.7.10-py3-none-any.whl (10.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.7/10.7 MB\u001b[0m \u001b[31m92.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub>=0.5.0\n",
            "  Downloading huggingface_hub-0.9.1-py3-none-any.whl (120 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.7/120.7 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic in /usr/local/lib/python3.7/dist-packages (from farm-haystack[colab]) (1.9.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from farm-haystack[colab]) (0.3.5.1)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from farm-haystack[colab]) (1.0.2)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from farm-haystack[colab]) (4.3.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from farm-haystack[colab]) (1.3.5)\n",
            "Requirement already satisfied: torch<1.13,>1.9 in /usr/local/lib/python3.7/dist-packages (from farm-haystack[colab]) (1.12.1+cu113)\n",
            "Collecting transformers==4.21.2\n",
            "  Downloading transformers-4.21.2-py3-none-any.whl (4.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m97.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting elasticsearch<7.11,>=7.7\n",
            "  Downloading elasticsearch-7.10.1-py2.py3-none-any.whl (322 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.1/322.1 kB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting posthog\n",
            "  Downloading posthog-2.1.2-py2.py3-none-any.whl (32 kB)\n",
            "Collecting rapidfuzz<2.8.0,>=2.0.15\n",
            "  Downloading rapidfuzz-2.7.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m79.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from farm-haystack[colab]) (4.64.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from farm-haystack[colab]) (2.6.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from farm-haystack[colab]) (3.7)\n",
            "Collecting python-docx\n",
            "  Downloading python-docx-0.8.11.tar.gz (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m89.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting requests\n",
            "  Downloading requests-2.28.1-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting grpcio==1.47.0\n",
            "  Downloading grpcio-1.47.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.7/dist-packages (from grpcio==1.47.0->farm-haystack[colab]) (1.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.21.2->farm-haystack[colab]) (3.8.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.21.2->farm-haystack[colab]) (1.21.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.21.2->farm-haystack[colab]) (2022.6.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.21.2->farm-haystack[colab]) (21.3)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m91.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.21.2->farm-haystack[colab]) (6.0)\n",
            "Collecting azure-core<2.0.0,>=1.23.0\n",
            "  Downloading azure_core-1.25.1-py3-none-any.whl (178 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.8/178.8 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-common~=1.1\n",
            "  Downloading azure_common-1.1.28-py2.py3-none-any.whl (14 kB)\n",
            "Collecting msrest>=0.6.21\n",
            "  Downloading msrest-0.7.1-py3-none-any.whl (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from azure-ai-formrecognizer>=3.2.0b2->farm-haystack[colab]) (4.1.1)\n",
            "Requirement already satisfied: urllib3<2,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from elasticsearch<7.11,>=7.7->farm-haystack[colab]) (1.24.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from elasticsearch<7.11,>=7.7->farm-haystack[colab]) (2022.6.15)\n",
            "Collecting jarowinkler<2.0.0,>=1.2.0\n",
            "  Downloading jarowinkler-1.2.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (113 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.7/113.7 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.7/dist-packages (from requests->farm-haystack[colab]) (2.1.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->farm-haystack[colab]) (2.10)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=1.0.0->farm-haystack[colab]) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=1.0.0->farm-haystack[colab]) (1.1.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers>=2.2.0->farm-haystack[colab]) (0.13.1+cu113)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m69.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->farm-haystack[colab]) (3.8.1)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->farm-haystack[colab]) (5.9.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->farm-haystack[colab]) (0.18.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->farm-haystack[colab]) (22.1.0)\n",
            "Requirement already satisfied: Flask<3 in /usr/local/lib/python3.7/dist-packages (from mlflow->farm-haystack[colab]) (1.1.4)\n",
            "Requirement already satisfied: pytz<2023 in /usr/local/lib/python3.7/dist-packages (from mlflow->farm-haystack[colab]) (2022.2.1)\n",
            "Collecting docker<7,>=4.0.0\n",
            "  Downloading docker-6.0.0-py3-none-any.whl (147 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.2/147.2 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting prometheus-flask-exporter<1\n",
            "  Downloading prometheus_flask_exporter-0.20.3-py3-none-any.whl (18 kB)\n",
            "Collecting databricks-cli<1,>=0.8.7\n",
            "  Downloading databricks-cli-0.17.3.tar.gz (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.7/dist-packages (from mlflow->farm-haystack[colab]) (7.1.2)\n",
            "Requirement already satisfied: sqlalchemy<2,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from mlflow->farm-haystack[colab]) (1.4.41)\n",
            "Collecting alembic<2\n",
            "  Downloading alembic-1.8.1-py3-none-any.whl (209 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.8/209.8 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting querystring-parser<2\n",
            "  Downloading querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\n",
            "Requirement already satisfied: cloudpickle<3 in /usr/local/lib/python3.7/dist-packages (from mlflow->farm-haystack[colab]) (1.5.0)\n",
            "Requirement already satisfied: entrypoints<1 in /usr/local/lib/python3.7/dist-packages (from mlflow->farm-haystack[colab]) (0.4)\n",
            "Requirement already satisfied: protobuf<5,>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from mlflow->farm-haystack[colab]) (3.17.3)\n",
            "Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from mlflow->farm-haystack[colab]) (0.4.2)\n",
            "Collecting gitpython<4,>=2.1.0\n",
            "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.2/181.2 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gunicorn<21\n",
            "  Downloading gunicorn-20.1.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->farm-haystack[colab]) (2.8.2)\n",
            "Collecting backoff<2.0.0,>=1.10.0\n",
            "  Downloading backoff-1.11.1-py2.py3-none-any.whl (13 kB)\n",
            "Collecting monotonic>=1.5\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: lxml>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from python-docx->farm-haystack[colab]) (4.9.1)\n",
            "Collecting num2words\n",
            "  Downloading num2words-0.5.12-py3-none-any.whl (125 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.2/125.2 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: inflect in /usr/local/lib/python3.7/dist-packages (from quantulum3->farm-haystack[colab]) (2.1.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tika->farm-haystack[colab]) (57.4.0)\n",
            "Collecting Mako\n",
            "  Downloading Mako-1.2.3-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyjwt>=1.7.0\n",
            "  Downloading PyJWT-2.5.0-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: oauthlib>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from databricks-cli<1,>=0.8.7->mlflow->farm-haystack[colab]) (3.2.0)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.7/dist-packages (from databricks-cli<1,>=0.8.7->mlflow->farm-haystack[colab]) (0.8.10)\n",
            "Collecting websocket-client>=0.32.0\n",
            "  Downloading websocket_client-1.4.1-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.0/55.0 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting urllib3<2,>=1.21.1\n",
            "  Downloading urllib3-1.26.12-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.4/140.4 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from Flask<3->mlflow->farm-haystack[colab]) (1.0.1)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from Flask<3->mlflow->farm-haystack[colab]) (1.1.0)\n",
            "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from Flask<3->mlflow->farm-haystack[colab]) (2.11.3)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting isodate>=0.6.0\n",
            "  Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests-oauthlib>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from msrest>=0.6.21->azure-ai-formrecognizer>=3.2.0b2->farm-haystack[colab]) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.21.2->farm-haystack[colab]) (3.0.9)\n",
            "Collecting prometheus-client\n",
            "  Downloading prometheus_client-0.14.1-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy<2,>=1.4.0->mlflow->farm-haystack[colab]) (1.1.3)\n",
            "Collecting docopt>=0.6.2\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence-transformers>=2.2.0->farm-haystack[colab]) (7.1.2)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->Flask<3->mlflow->farm-haystack[colab]) (2.0.1)\n",
            "Building wheels for collected packages: sentence-transformers, farm-haystack, langdetect, python-docx, seqeval, tika, databricks-cli, docopt\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125938 sha256=7c9c703b961e6d286f3cb842c047b6190b3c656895a9c770576eeda9820880a0\n",
            "  Stored in directory: /root/.cache/pip/wheels/bf/06/fb/d59c1e5bd1dac7f6cf61ec0036cc3a10ab8fecaa6b2c3d3ee9\n",
            "  Building wheel for farm-haystack (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for farm-haystack: filename=farm_haystack-1.10.0rc0-py3-none-any.whl size=682724 sha256=68f2938044042580835d4077f8d9c53abb682dc5196e034beea3654a1edb5942\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-9uxjhv5z/wheels/a7/05/3b/9b33368d9af06a39f8e6af2e97fa2af876e893ade323cfc2c9\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993242 sha256=442ab8c536a10312b78108d0fb9ef93352466b97457a95550e6584c0a878141f\n",
            "  Stored in directory: /root/.cache/pip/wheels/c5/96/8a/f90c59ed25d75e50a8c10a1b1c2d4c402e4dacfa87f3aff36a\n",
            "  Building wheel for python-docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-docx: filename=python_docx-0.8.11-py3-none-any.whl size=184507 sha256=c0a5ec63f14f421adf0c91d9d1738691fcd5899866239c8bff1a6f3fb8d8f54f\n",
            "  Stored in directory: /root/.cache/pip/wheels/f6/6f/b9/d798122a8b55b74ad30b5f52b01482169b445fbb84a11797a6\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16180 sha256=cfe16db918dc4c19d91ecaac2e24636ba2a4535635a0fb8dec5d9f613d864cbb\n",
            "  Stored in directory: /root/.cache/pip/wheels/05/96/ee/7cac4e74f3b19e3158dce26a20a1c86b3533c43ec72a549fd7\n",
            "  Building wheel for tika (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tika: filename=tika-1.24-py3-none-any.whl size=32893 sha256=c627cec790e57584cdca147b4330cbc037d65f3160532b692a4ae982f01f950d\n",
            "  Stored in directory: /root/.cache/pip/wheels/ec/2b/38/58ff05467a742e32f67f5d0de048fa046e764e2fbb25ac93f3\n",
            "  Building wheel for databricks-cli (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for databricks-cli: filename=databricks_cli-0.17.3-py3-none-any.whl size=139102 sha256=8705eafb409d57bfcd614e24ed49e1d61cb24b486d61e88f1ba1e283e3c731d6\n",
            "  Stored in directory: /root/.cache/pip/wheels/3f/73/87/c1e4b2145eb6049bb6c9aaf7ea1e38302b77ca219b6fef5d5c\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13723 sha256=f2b7ee804342b7737a9e32b22b8087f2f0372aba37f58b063bc93600cd8e915d\n",
            "  Stored in directory: /root/.cache/pip/wheels/72/b0/3f/1d95f96ff986c7dfffe46ce2be4062f38ebd04b506c77c81b9\n",
            "Successfully built sentence-transformers farm-haystack langdetect python-docx seqeval tika databricks-cli docopt\n",
            "Installing collected packages: tokenizers, sentencepiece, monotonic, mmh3, docopt, azure-common, websocket-client, urllib3, smmap, querystring-parser, python-docx, pyjwt, prometheus-client, num2words, langdetect, jarowinkler, isodate, gunicorn, grpcio, backoff, requests, rapidfuzz, quantulum3, Mako, gitdb, elasticsearch, elastic-apm, tika, seqeval, prometheus-flask-exporter, posthog, huggingface-hub, gitpython, docker, databricks-cli, azure-core, alembic, transformers, msrest, mlflow, sentence-transformers, azure-ai-formrecognizer, farm-haystack\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: grpcio\n",
            "    Found existing installation: grpcio 1.48.1\n",
            "    Uninstalling grpcio-1.48.1:\n",
            "      Successfully uninstalled grpcio-1.48.1\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "Successfully installed Mako-1.2.3 alembic-1.8.1 azure-ai-formrecognizer-3.2.0 azure-common-1.1.28 azure-core-1.25.1 backoff-1.11.1 databricks-cli-0.17.3 docker-6.0.0 docopt-0.6.2 elastic-apm-6.12.0 elasticsearch-7.10.1 farm-haystack-1.10.0rc0 gitdb-4.0.9 gitpython-3.1.27 grpcio-1.47.0 gunicorn-20.1.0 huggingface-hub-0.9.1 isodate-0.6.1 jarowinkler-1.2.2 langdetect-1.0.9 mlflow-1.29.0 mmh3-3.0.0 monotonic-1.6 msrest-0.7.1 num2words-0.5.12 posthog-2.1.2 prometheus-client-0.14.1 prometheus-flask-exporter-0.20.3 pyjwt-2.5.0 python-docx-0.8.11 quantulum3-0.7.10 querystring-parser-1.2.4 rapidfuzz-2.7.0 requests-2.28.1 sentence-transformers-2.2.2 sentencepiece-0.1.97 seqeval-1.2.2 smmap-5.0.0 tika-1.24 tokenizers-0.12.1 transformers-4.21.2 urllib3-1.26.12 websocket-client-1.4.1\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Relevante bibliotheken importieren\n",
        "import pandas as pd\n",
        "import json\n",
        "from urllib.request import urlopen\n",
        "from haystack.utils import SquadData"
      ],
      "metadata": {
        "id": "ZQ_k89kmlILW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Erstellen einer Methode zum umwandeln der Span Based TrainingsDatensätze (von Gupta et al.) in das SQuaD format\n",
        "#drei parameter: \n",
        "#   url (string), die URL, von dem der Datensatz herutergelden wird\n",
        "#   ouputfile_name (string), der name der aufbereiteten json-Datei, die ausgegeben wird\n",
        "#   split (string), angabe ob der datensatz der train split (\"train\") ist oder,\n",
        "#     im fall einer der anderen splits (test, val), welche span heuristik verwendet werden soll \n",
        "#     (\"answers_snippet_spans_bleu2\",\"answers_snippet_spans_bleu4\",\"answers_snippet_spans_rouge\",\"answers_sentence_ir\")\n",
        "def amazonQA_span_based_to_squad(url,outputfile_name,split):\n",
        "    data = []\n",
        "\n",
        "    if(split==\"train\"):\n",
        "      answer_field='answers'\n",
        "    else:\n",
        "      answer_field=split\n",
        "\n",
        "    #für jedes json objekt in der jsonl die relevanten attribute der liste \"data\" hinzufüen\n",
        "    with urlopen(url) as f:\n",
        "        for line in f:\n",
        "          json_line = json.loads(line)\n",
        "          for question in json_line['qas']:\n",
        "            for answer in question[answer_field]:\n",
        "                data.append([json_line['context'][0:10], json_line['context'],question['question'], question['id'], answer['text'], answer['answer_start'], question['is_impossible']])\n",
        "    \n",
        "    #aus der liste \"data\" wird ein pandas Dataframe df erstellt, bei dem die spaltennamen dem Squad Format entsprechen (diese Spaltennamen müssen so vorhanden sein, siehe https://github.com/deepset-ai/haystack/issues/2119)\n",
        "    df=pd.DataFrame(data, columns=[\"title\", \"context\", \"question\", \"id\", \"answer_text\", \"answer_start\", \"is_impossible\"])\n",
        "\n",
        "    # konvertierung zu einem SquadData Objekt, basierend auf https://github.com/deepset-ai/haystack/issues/2119 (der meiste code wurde hier entnommen)\n",
        "    # und https://github.com/deepset-ai/haystack/issues/2651 (um den fehler aus der ersten quelle [sq.df_to_squad(df)] zu korrigieren [richtig: sq.df_to_data(df)])\n",
        "    sq = SquadData({'data': {}})\n",
        "    squad_docs = sq.df_to_data(df)\n",
        "    converted_squad = SquadData(squad_docs)\n",
        "    converted_squad.save(outputfile_name + '.json')\n"
      ],
      "metadata": {
        "id": "Asd0Dy02vfxw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dieser Datensatz ist viel zu groß, deshalb eine alternative methode, die nur jede zehnte frage hernimmt, um so einen downgesampleten datensatz zu erhalten:"
      ],
      "metadata": {
        "id": "CSvwWDyAtmrO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def downsample_amazonQA_span_based_to_squad(url,outputfile_name,split):\n",
        "    data = []\n",
        "\n",
        "    if(split==\"train\"):\n",
        "      answer_field='answers'\n",
        "    else:\n",
        "      answer_field=split\n",
        "\n",
        "    #für jedes json objekt in der jsonl die relevanten attribute der liste \"data\" hinzufüen\n",
        "    with urlopen(url) as f:\n",
        "        for i, line in enumerate(f):\n",
        "          if i % 10 == 0:\n",
        "            json_line = json.loads(line)\n",
        "            for question in json_line['qas']:\n",
        "              for answer in question[answer_field]:\n",
        "                  data.append([json_line['context'][0:10], json_line['context'],question['question'], question['id'], answer['text'], answer['answer_start'], question['is_impossible']])\n",
        "    \n",
        "    #aus der liste \"data\" wird ein pandas Dataframe df erstellt, bei dem die spaltennamen dem Squad Format entsprechen (diese Spaltennamen müssen so vorhanden sein, siehe https://github.com/deepset-ai/haystack/issues/2119)\n",
        "    df=pd.DataFrame(data, columns=[\"title\", \"context\", \"question\", \"id\", \"answer_text\", \"answer_start\", \"is_impossible\"])\n",
        "    sq = SquadData({'data': {}})\n",
        "    squad_docs = sq.df_to_data(df)\n",
        "    converted_squad = SquadData(squad_docs)      \n",
        "    converted_squad.save(outputfile_name + '.json')\n",
        "    # konvertierung zu einem SquadData Objekt, basierend auf https://github.com/deepset-ai/haystack/issues/2119 (der meiste code wurde hier entnommen)\n",
        "    # und https://github.com/deepset-ai/haystack/issues/2651 (um den fehler aus der ersten quelle [sq.df_to_squad(df)] zu korrigieren [richtig: sq.df_to_data(df)])"
      ],
      "metadata": {
        "id": "ZvyT_K1btjf5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#URL für den trainingssplitdes Span based Datensatzes von Gupta et al. (siehe https://github.com/amazonqa/amazonqa)\n",
        "url_train = \"https://amazon-qa.s3-us-west-2.amazonaws.com/train-qar_squad.jsonl\"\n",
        "\n",
        "#Aufbereitung des Trainingsdatensatz mit der vorher definierten Methode \"amazonQA_span_based_to_squad()\":\n",
        "\n",
        "downsample_amazonQA_span_based_to_squad(url_train,\"train_split_squad_format\",\"train\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bsDDYeKVnbsj",
        "outputId": "e673c30e-1c23-4490-8ed5-b3bd3b57c1a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:haystack.utils.squad_data:Converting data frame to squad format data\n",
            "INFO:haystack.utils.squad_data:Aggregating the answers of each question\n",
            "100%|██████████| 45594/45594 [01:02<00:00, 730.94it/s]\n",
            "INFO:haystack.utils.squad_data:Aggregating the questions of each paragraphs of each document\n",
            "100%|██████████| 45520/45520 [00:42<00:00, 1070.55it/s]\n",
            "INFO:haystack.utils.squad_data:Aggregating the paragraphs of each document\n",
            "100%|██████████| 20062/20062 [00:15<00:00, 1335.83it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#zippen der datei für schnelleres herunterladen\n",
        "!zip -r \"./train_split_squad_format_downsampled.zip\" \"./train_split_squad_format.json\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2Nq-E2OJdQg",
        "outputId": "a696e50f-f9cc-42ec-e7f5-dc3113f728b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: train_split_squad_format.json (deflated 69%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#URL für den validationsplit des Span based Datensatzes von Gupta et al. (siehe https://github.com/amazonqa/amazonqa)\n",
        "url_val = \"https://amazon-qa.s3-us-west-2.amazonaws.com/val-qar_squad.jsonl\"\n",
        "\n",
        "#Aufbereitung des Validationsdatensatz mit der vorher definierten Methode \"amazonQA_span_based_to_squad()\":\n",
        "\n",
        "#wir verwenden als span heuristik hier \"answers_sentence_ir\", da die ergebnisse von Gupta et al. (2019) zeigten, das die IR based score match die effektivste\n",
        "#span Heuristik für den Amazon QA Datensatz ist\n",
        "downsample_amazonQA_span_based_to_squad(url_val,\"val_split_squad_format\",\"answers_sentence_ir\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LwGpHmLaMOqX",
        "outputId": "035d7c50-d037-46b2-e23c-a67ae719a52d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:haystack.utils.squad_data:Converting data frame to squad format data\n",
            "INFO:haystack.utils.squad_data:Aggregating the answers of each question\n",
            "100%|██████████| 5897/5897 [00:07<00:00, 745.93it/s]\n",
            "INFO:haystack.utils.squad_data:Aggregating the questions of each paragraphs of each document\n",
            "100%|██████████| 5893/5893 [00:05<00:00, 1042.16it/s]\n",
            "INFO:haystack.utils.squad_data:Aggregating the paragraphs of each document\n",
            "100%|██████████| 3787/3787 [00:02<00:00, 1332.49it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#zippen der datei für schnelleres herunterladen\n",
        "!zip -r \"./val_split_squad_format_downsampled.zip\" \"./val_split_squad_format.json\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8wHZfngGMvEZ",
        "outputId": "ee37a812-3595-49f1-b779-ab142993fe35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: val_split_squad_format.json (deflated 70%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#URL für den testssplit des Span based Datensatzes von Gupta et al. (siehe https://github.com/amazonqa/amazonqa)\n",
        "url_test = \"https://drive.google.com/u/0/uc?id=1eede6X_r7uoOmDZkv5NlbM4Mu-OP-cCe&export=download&confirm=t\"\n",
        "\n",
        "#Aufbereitung des Testdatensatz mit der vorher definierten Methode \"amazonQA_span_based_to_squad()\":\n",
        "\n",
        "#wir verwenden als span heuristik hier \"answers_sentence_ir\", da die ergebnisse von Gupta et al. (2019) zeigten, das die IR based score match die effektivste\n",
        "#span Heuristik für den Amazon QA Datensatz ist\n",
        "downsample_amazonQA_span_based_to_squad(url_test,\"test_split_squad_format\",\"answers_sentence_ir\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bTk4q6MTDJBn",
        "outputId": "4637ea03-0381-4526-e641-cd6dfaa68ac0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:haystack.utils.squad_data:Converting data frame to squad format data\n",
            "INFO:haystack.utils.squad_data:Aggregating the answers of each question\n",
            "100%|██████████| 5953/5953 [00:07<00:00, 751.01it/s]\n",
            "INFO:haystack.utils.squad_data:Aggregating the questions of each paragraphs of each document\n",
            "100%|██████████| 5947/5947 [00:05<00:00, 1113.83it/s]\n",
            "INFO:haystack.utils.squad_data:Aggregating the paragraphs of each document\n",
            "100%|██████████| 3818/3818 [00:02<00:00, 1347.31it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#zippen der datei für schnelleres herunterladen\n",
        "!zip -r \"./test_split_squad_format_downsampled.zip\" \"./test_split_squad_format.json\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXYm5ZVOMyIs",
        "outputId": "da0b5f0c-2495-4d1a-bad7-180f2bb0bd2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: test_split_squad_format.json (deflated 70%)\n"
          ]
        }
      ]
    }
  ]
}